{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Internet News Data","metadata":{}},{"cell_type":"markdown","source":"#### By: Dana Cassidy","metadata":{}},{"cell_type":"markdown","source":"## In this notebook, I'll be building both a reccomendation engine for news articles and also using NLP to predict trends in the news. I'll be doing both to gain a better understanding of how news is shared on the internet and which metrics -- such as engagement, post times, and more -- play a role into publication. \n\n### * I'll also offer explanatory analysis to better contextualize the data and draw conclusions from there, based on both the statistics and my experience as a journalist working in professional newsrooms. *","metadata":{}},{"cell_type":"markdown","source":"## Outline:\n### * 1) Prepping the data\n### * 2) Reccomendation engine using Kmeans and TF-IDF\n### * 3) EDA on trends and metrics\n### * 4) Predicting using NLP\n\n","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nfrom wordcloud import WordCloud\nfrom xgboost import XGBClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-09T16:08:58.19201Z","iopub.execute_input":"2021-06-09T16:08:58.192561Z","iopub.status.idle":"2021-06-09T16:08:59.946911Z","shell.execute_reply.started":"2021-06-09T16:08:58.192431Z","shell.execute_reply":"2021-06-09T16:08:59.946132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Let's begin by importing the data:","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/internet-articles-data-with-users-engagement/articles_data.csv')\ndf = df.drop('Unnamed: 0', axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-06-09T16:10:14.552281Z","iopub.execute_input":"2021-06-09T16:10:14.552664Z","iopub.status.idle":"2021-06-09T16:10:14.709765Z","shell.execute_reply.started":"2021-06-09T16:10:14.552631Z","shell.execute_reply":"2021-06-09T16:10:14.7087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-09T16:10:14.960573Z","iopub.execute_input":"2021-06-09T16:10:14.960948Z","iopub.status.idle":"2021-06-09T16:10:14.984838Z","shell.execute_reply.started":"2021-06-09T16:10:14.960915Z","shell.execute_reply":"2021-06-09T16:10:14.983502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Upon examining our data, we can see that there are standard metrics for examining what the content actually is, such as sources, authors, titles, a description of the piece, urls, and date of publication. We can also see that there are columns that determine if it is a top article, the count of engagement reactions, comments, shares, and plugins. This will all be useful in our analysis and predictions.","metadata":{}},{"cell_type":"markdown","source":"## Preparing our data","metadata":{}},{"cell_type":"raw","source":"Our data has some missing values. Let's fill the null values of the data. We already dropped the \"Unnamed\" column to better clean our dataframe.","metadata":{}},{"cell_type":"code","source":"df['title'] = df['title'].fillna('NaN')\ndf['description'] = df['description'].fillna('NaN')\ndf['content'] = df['content'].fillna('NaN')\ndf['published_at'] = df['published_at'].fillna('NaN')\n\ndf['engagement_reaction_count'] = df['engagement_reaction_count'].fillna(0)\ndf['engagement_comment_count'] = df['engagement_comment_count'].fillna(0)\ndf['engagement_share_count'] = df['engagement_share_count'].fillna(0)\ndf['engagement_comment_plugin_count'] = df['engagement_comment_plugin_count'].fillna(0)","metadata":{"execution":{"iopub.status.busy":"2021-06-09T16:13:19.679288Z","iopub.execute_input":"2021-06-09T16:13:19.679742Z","iopub.status.idle":"2021-06-09T16:13:19.69923Z","shell.execute_reply.started":"2021-06-09T16:13:19.679708Z","shell.execute_reply":"2021-06-09T16:13:19.698065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's check our head again.","metadata":{}},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-09T16:13:34.100332Z","iopub.execute_input":"2021-06-09T16:13:34.100704Z","iopub.status.idle":"2021-06-09T16:13:34.125393Z","shell.execute_reply.started":"2021-06-09T16:13:34.100671Z","shell.execute_reply":"2021-06-09T16:13:34.124327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our data is much easier to use now.","metadata":{}},{"cell_type":"code","source":"print(df['source_id'].unique())\nprint(df['source_name'].unique())","metadata":{"execution":{"iopub.status.busy":"2021-06-09T16:22:43.688643Z","iopub.execute_input":"2021-06-09T16:22:43.689029Z","iopub.status.idle":"2021-06-09T16:22:43.696334Z","shell.execute_reply.started":"2021-06-09T16:22:43.688975Z","shell.execute_reply":"2021-06-09T16:22:43.695535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are two columns, 'source_name' and 'source_id', which are almost identical. I'm also not sure why the value 460 is in there. Let's drop the row that contains the 460:","metadata":{}},{"cell_type":"code","source":"print(df[df['source_name']==\"460.0\"])\n#Since its all full of NAN value we'll drop this useless row\ndf = df[df['source_name']!=\"460.0\"]\nprint(\"Row Dropped\")\ndf.shape","metadata":{"execution":{"iopub.status.busy":"2021-06-09T16:24:00.243097Z","iopub.execute_input":"2021-06-09T16:24:00.245583Z","iopub.status.idle":"2021-06-09T16:24:00.272986Z","shell.execute_reply.started":"2021-06-09T16:24:00.245524Z","shell.execute_reply":"2021-06-09T16:24:00.271725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2021-06-09T16:24:53.767154Z","iopub.execute_input":"2021-06-09T16:24:53.767543Z","iopub.status.idle":"2021-06-09T16:24:53.78659Z","shell.execute_reply.started":"2021-06-09T16:24:53.767512Z","shell.execute_reply":"2021-06-09T16:24:53.785279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There seems to be about 24 or so rows without a description. Let's clean that up:","metadata":{}},{"cell_type":"code","source":"print(df[df['description'].isna()].isna().sum())\ndf = df[~df['title'].isna()]\ndf_2 = df.copy()","metadata":{"execution":{"iopub.status.busy":"2021-06-09T16:25:25.641543Z","iopub.execute_input":"2021-06-09T16:25:25.641882Z","iopub.status.idle":"2021-06-09T16:25:25.65882Z","shell.execute_reply.started":"2021-06-09T16:25:25.641852Z","shell.execute_reply":"2021-06-09T16:25:25.657651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will be dropping the 18 without contents because we're not really able to improve or fix them since we aren't sure what the contents of the article entirely is.","metadata":{}},{"cell_type":"code","source":"df = df_2.copy()\nempty_desc = df[df['description'].isna()]\ndf = df[~df['description'].isna()]\nempty_desc = empty_desc[~empty_desc['content'].isna()]\ndf = pd.concat([df,empty_desc],axis=0)\ndf.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2021-06-09T16:26:22.052896Z","iopub.execute_input":"2021-06-09T16:26:22.053287Z","iopub.status.idle":"2021-06-09T16:26:22.08411Z","shell.execute_reply.started":"2021-06-09T16:26:22.053249Z","shell.execute_reply":"2021-06-09T16:26:22.083399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Now, let's use KMeans via TF-IDF","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer #The Vector creator\nfrom sklearn.metrics.pairwise import linear_kernel #Cosine similarity\nfrom sklearn.cluster import MiniBatchKMeans #Kmeans Clustering Batch","metadata":{"execution":{"iopub.status.busy":"2021-06-09T16:37:38.483546Z","iopub.execute_input":"2021-06-09T16:37:38.48408Z","iopub.status.idle":"2021-06-09T16:37:38.488182Z","shell.execute_reply.started":"2021-06-09T16:37:38.484046Z","shell.execute_reply":"2021-06-09T16:37:38.48718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cluster_the_content = df['description']\nvector = TfidfVectorizer(max_df=0.5,min_df=1,stop_words=\"english\",lowercase=True,use_idf=True,norm=u'l2',smooth_idf=True)\ntfidf = vector.fit_transform(cluster_the_content)","metadata":{"execution":{"iopub.status.busy":"2021-06-09T16:38:11.005382Z","iopub.execute_input":"2021-06-09T16:38:11.005692Z","iopub.status.idle":"2021-06-09T16:38:11.405694Z","shell.execute_reply.started":"2021-06-09T16:38:11.005665Z","shell.execute_reply":"2021-06-09T16:38:11.40469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"k = 200\nkmeans = MiniBatchKMeans(n_clusters= k)\nkmeans.fit(tfidf)\ncenters = kmeans.cluster_centers_.argsort()[:,::-1]\nterms = vector.get_feature_names()","metadata":{"execution":{"iopub.status.busy":"2021-06-09T16:38:58.624893Z","iopub.execute_input":"2021-06-09T16:38:58.625272Z","iopub.status.idle":"2021-06-09T16:39:01.829678Z","shell.execute_reply.started":"2021-06-09T16:38:58.625242Z","shell.execute_reply":"2021-06-09T16:39:01.828731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Let's add our descriptions to predict their classes:","metadata":{}},{"cell_type":"code","source":"request_transform = vector.transform(df['description'])\ndf['cluster'] = kmeans.predict(request_transform)\ndf['cluster'].value_counts().head()","metadata":{"execution":{"iopub.status.busy":"2021-06-09T16:39:34.397301Z","iopub.execute_input":"2021-06-09T16:39:34.397653Z","iopub.status.idle":"2021-06-09T16:39:35.668903Z","shell.execute_reply.started":"2021-06-09T16:39:34.397624Z","shell.execute_reply":"2021-06-09T16:39:35.667819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-09T16:39:48.544101Z","iopub.execute_input":"2021-06-09T16:39:48.544478Z","iopub.status.idle":"2021-06-09T16:39:48.567729Z","shell.execute_reply.started":"2021-06-09T16:39:48.544429Z","shell.execute_reply":"2021-06-09T16:39:48.566698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Let's find the similarity between the documents:","metadata":{}},{"cell_type":"code","source":"def find_similar(matrix,index,top_n=5):\n    cosine_similarities = linear_kernel(matrix[index:index+1],matrix).flatten()\n    related_docs_indices = [i for i in cosine_similarities.argsort()[::-1] if i != index]\n    return [index for index in related_docs_indices][0:top_n]","metadata":{"execution":{"iopub.status.busy":"2021-06-09T16:42:08.260442Z","iopub.execute_input":"2021-06-09T16:42:08.260923Z","iopub.status.idle":"2021-06-09T16:42:08.265749Z","shell.execute_reply.started":"2021-06-09T16:42:08.260891Z","shell.execute_reply":"2021-06-09T16:42:08.26512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Graphing","metadata":{}},{"cell_type":"markdown","source":"Edges are :\n* Wrote : relation between title and person\n* CAT : Relation between title and Press\n* Description : Relation between cluster and a movie\n* Similarity in sense of description","metadata":{}},{"cell_type":"code","source":"import networkx as nx \nimport time\n\nG = nx.Graph(label=\"Article\")\nstart_time = time.time()\nfor i,rowi in df.iterrows() :\n    if (i > 3000) :\n        continue\n    if (i%1000 == 0) : \n        print(\"Iter  {} --- {} secondes --\".format(i,time.time()-start_time))\n    G.add_node(rowi['title'],key=i,label=\"Article\")\n    G.add_node(rowi['author'],label=\"Person\")\n    G.add_edge(rowi['title'],rowi['author'],label=\"Wrote\")\n    G.add_node(rowi['source_name'],label=\"Press\")\n    G.add_edge(rowi['title'],rowi['source_name'],label=\"CAT\")\n    #Similarity Node :\n    indices = find_similar(tfidf, i, top_n = 5)\n    snode=\"Sim(\"+rowi['title'][:15].strip()+\")\"        \n    G.add_node(snode,label=\"SIMILAR\")\n    G.add_edge(rowi['title'], snode, label=\"SIMILARITY\")\n    for element in indices:\n        G.add_edge(snode, df['title'].iloc[element], label=\"SIMILARITY\")\nprint(\" finish -- {} seconds --\".format(time.time() - start_time))   ","metadata":{"execution":{"iopub.status.busy":"2021-06-09T16:42:38.780191Z","iopub.execute_input":"2021-06-09T16:42:38.780803Z","iopub.status.idle":"2021-06-09T16:43:08.790947Z","shell.execute_reply.started":"2021-06-09T16:42:38.780766Z","shell.execute_reply":"2021-06-09T16:43:08.78984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_all_adj_nodes(list_in):\n    sub_graph=set()\n    for m in list_in:\n        sub_graph.add(m)\n        for e in G.neighbors(m):        \n                sub_graph.add(e)\n    return list(sub_graph)","metadata":{"execution":{"iopub.status.busy":"2021-06-09T16:44:23.48565Z","iopub.execute_input":"2021-06-09T16:44:23.486037Z","iopub.status.idle":"2021-06-09T16:44:23.491173Z","shell.execute_reply.started":"2021-06-09T16:44:23.485995Z","shell.execute_reply":"2021-06-09T16:44:23.490074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def draw_sub_graph(sub_graph):\n    subgraph = G.subgraph(sub_graph)\n    colors=[]\n    for e in subgraph.nodes():\n        if G.nodes[e]['label']==\"Article\":\n            colors.append('blue')\n        elif G.nodes[e]['label']==\"Person\":\n            colors.append('red')\n        elif G.nodes[e]['label']==\"Press\":\n            colors.append('green')\n        elif G.nodes[e]['label']==\"SIMILAR\":\n            colors.append('yellow')\n    nx.draw(subgraph, with_labels=True, font_weight='bold',node_color=colors)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-09T16:44:48.91145Z","iopub.execute_input":"2021-06-09T16:44:48.911782Z","iopub.status.idle":"2021-06-09T16:44:48.918515Z","shell.execute_reply.started":"2021-06-09T16:44:48.911754Z","shell.execute_reply":"2021-06-09T16:44:48.917433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Examining our graph with two examples to visualize:","metadata":{}},{"cell_type":"code","source":"list_in=[df['title'].loc[1],df['title'].loc[2]]\nsub_graph = get_all_adj_nodes(list_in)\ndraw_sub_graph(sub_graph)\n","metadata":{"execution":{"iopub.status.busy":"2021-06-09T16:45:15.124347Z","iopub.execute_input":"2021-06-09T16:45:15.124704Z","iopub.status.idle":"2021-06-09T16:45:15.275301Z","shell.execute_reply.started":"2021-06-09T16:45:15.124673Z","shell.execute_reply":"2021-06-09T16:45:15.274343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### The next function is going to grab the neighbors nodes within our graph and then compute the weight. We are then going to sort the neighbors by a weight value:","metadata":{}},{"cell_type":"code","source":"def get_recommendation(root):\n    commons_dict = {}\n    for e in G.neighbors(root):\n        for e2 in G.neighbors(e):\n            if e2==root:\n                continue\n            try :\n                if G.nodes[e2]['label']==\"Article\":\n                    commons = commons_dict.get(e2)\n                    if commons==None:\n                        commons_dict.update({e2 : [e]})\n                    else:\n                        commons.append(e)\n                        commons_dict.update({e2 : commons})\n            except :\n                pass\n    articles=[]\n    weight=[]\n    for key, values in commons_dict.items():\n        w=0.0\n        for e in values:\n            w=w+1/math.log(G.degree(e))\n        articles.append(key) \n        weight.append(w)\n    \n    result = pd.Series(data=np.array(weight),index=articles)\n    result.sort_values(inplace=True,ascending=False)        \n    return result;","metadata":{"execution":{"iopub.status.busy":"2021-06-09T16:46:17.917573Z","iopub.execute_input":"2021-06-09T16:46:17.917927Z","iopub.status.idle":"2021-06-09T16:46:17.926203Z","shell.execute_reply.started":"2021-06-09T16:46:17.917898Z","shell.execute_reply":"2021-06-09T16:46:17.925486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import math \n\nresult = get_recommendation(df['title'].loc[40])\nprint(\"*\"*40+\"\\n Recommendation for :\"+str(df['title'].loc[40])+\"\\n\"+\"*\"*40)\nprint(result.head())","metadata":{"execution":{"iopub.status.busy":"2021-06-09T16:46:48.476761Z","iopub.execute_input":"2021-06-09T16:46:48.477312Z","iopub.status.idle":"2021-06-09T16:46:48.488036Z","shell.execute_reply.started":"2021-06-09T16:46:48.477279Z","shell.execute_reply":"2021-06-09T16:46:48.486778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Analyzing and understanding","metadata":{}},{"cell_type":"markdown","source":"#### Our model was able to find a reccomendation and compare it with other articles based on the selected article at hand. For the content titled 'Pence defends decision to stay at Trump Doonbeg property', we can see other articles reccomended below it that are similar to the category and topic of the reccommended piece. This is the heart of how reccomendation algorithms work.","metadata":{}},{"cell_type":"markdown","source":"Let's do some further visual analysis on our data:","metadata":{}},{"cell_type":"code","source":"def bar_charts(title, x, y, colour, values, keys, figsize=(10, 5), fontsize=12):\n    fig, ax = plt.subplots(1, 1, figsize=figsize)\n    bars = plt.bar(keys, values, color=colour)\n\n    for bar in bars:\n        label = list(count)[list(bars).index(bar)]\n        height = bar.get_height()\n        plt.text(bar.get_x() + bar.get_width()/2, height, label, ha='center', va='bottom', \n                 fontsize=fontsize)\n\n    plt.title(title, fontsize=fontsize)\n    plt.xlabel(x, fontsize=fontsize)\n    plt.ylabel(y, fontsize=fontsize)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-09T16:50:03.36621Z","iopub.execute_input":"2021-06-09T16:50:03.366604Z","iopub.status.idle":"2021-06-09T16:50:03.375005Z","shell.execute_reply.started":"2021-06-09T16:50:03.366564Z","shell.execute_reply":"2021-06-09T16:50:03.373867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" def plots(title, x, y, values, keys, figsize=(10, 5), fontsize=12):\n    fig, ax = plt.subplots(1, 1, figsize=figsize)\n    plt.plot(list(keys), list(values))\n    plt.title(title)\n    plt.xlabel(x)\n    plt.ylabel(y)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-09T16:50:06.419049Z","iopub.execute_input":"2021-06-09T16:50:06.419444Z","iopub.status.idle":"2021-06-09T16:50:06.425431Z","shell.execute_reply.started":"2021-06-09T16:50:06.419409Z","shell.execute_reply":"2021-06-09T16:50:06.424416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I just made some functions for plotting both bar and line graphs, respectively. Let's see how our charts look:","metadata":{}},{"cell_type":"markdown","source":"The first variable we displayed is the \"source_name\" column, which is the origin of where the article is published. ","metadata":{}},{"cell_type":"code","source":"count = Counter(df['source_name'])\ncount = pd.Series(count).sort_values(ascending=False)\n\nkeys = list(count.keys())\nkeys[keys.index('The New York Times')] =  'The NY Times'\nkeys[keys.index('Al Jazeera English')] = 'Al Jazeera'\nkeys[keys.index('The Wall Street Journal')] =  'Wall Street Journal'\n\nbar_charts('Articles per source', 'Source name', 'Number of articles', 'blue', count, keys,\n          (20, 13), 18)","metadata":{"execution":{"iopub.status.busy":"2021-06-09T16:50:44.981983Z","iopub.execute_input":"2021-06-09T16:50:44.982386Z","iopub.status.idle":"2021-06-09T16:50:45.254662Z","shell.execute_reply.started":"2021-06-09T16:50:44.982353Z","shell.execute_reply":"2021-06-09T16:50:45.253771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It looks like \"Reuters\", \"BBC News\", and \"The Irish Times\" all have the most artices.","metadata":{}},{"cell_type":"markdown","source":"We can also show the distribution of how many articles were written throughout the year. We can select different timeframes:","metadata":{}},{"cell_type":"code","source":"month = [i[5:7] for i in df['published_at']]\ncount = Counter(month)\ncount = pd.Series(count).sort_values(ascending=False)[:2]\n\nbar_charts('Distribution of articles released per month', 'Month number', 'Number of articles',\n          'orange', count, count.keys(), figsize=(15, 10))","metadata":{"execution":{"iopub.status.busy":"2021-06-09T16:52:15.935613Z","iopub.execute_input":"2021-06-09T16:52:15.935982Z","iopub.status.idle":"2021-06-09T16:52:16.098119Z","shell.execute_reply.started":"2021-06-09T16:52:15.935952Z","shell.execute_reply":"2021-06-09T16:52:16.097113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can do the same for days:","metadata":{}},{"cell_type":"code","source":"day = [i[8:10] for i in df['published_at']]\ncount = Counter(day)\ncount = pd.Series(count).sort_values(ascending=False)[:13]\n\nbar_charts('Day that articles were released', 'Day released', 'Number of articles', 'purple', \n           count, count.keys(), figsize=(15, 10))","metadata":{"execution":{"iopub.status.busy":"2021-06-09T16:52:58.438878Z","iopub.execute_input":"2021-06-09T16:52:58.439264Z","iopub.status.idle":"2021-06-09T16:52:58.684603Z","shell.execute_reply.started":"2021-06-09T16:52:58.439232Z","shell.execute_reply":"2021-06-09T16:52:58.683648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It's interesting to see how \"03\" was the most popular day of release. Journalistically, I am not sure why. Perhaps it's because a lot of news events happen on certain days of the third, like the U.S. presidential election. This doesn't explain why for every month, though.","metadata":{}},{"cell_type":"markdown","source":"Let's take it even further and look at the hours in terms of which are the most popular for publishing news articles.","metadata":{}},{"cell_type":"code","source":"hour = [i[11:13] for i in df['published_at']]\ncount = Counter(hour)\ncount = pd.Series(count).sort_values(ascending=False)[:20]\n\nbar_charts('Hours that articles were released', 'Hour released', 'Number of articles',\n          'green', count, count.keys(), figsize=(13, 10))","metadata":{"execution":{"iopub.status.busy":"2021-06-09T16:53:33.998631Z","iopub.execute_input":"2021-06-09T16:53:33.999011Z","iopub.status.idle":"2021-06-09T16:53:34.325649Z","shell.execute_reply.started":"2021-06-09T16:53:33.998972Z","shell.execute_reply":"2021-06-09T16:53:34.324535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A lot of articles seem to be primarily published between 2-4 p.m. This is fascinating because in newsrooms a lot of bigger feature pieces are published at the beginning of a work day, around 9 a.m., so people have time to read it during the day. It also, however, makes sense the the most *popular* upload time overall is the afternoon because breaking news typically happens in the morning and then articles follow it soon after. Meetings and other events can happen during the day, with correpsonding news about such events.","metadata":{}},{"cell_type":"markdown","source":"We can also conduct analysis on line graphs to see how many articles were released:","metadata":{}},{"cell_type":"code","source":"day_and_month = pd.DataFrame([])\nday_and_month['day'] = day\nday_and_month['month'] = month\n\ncount1 = Counter(day_and_month[day_and_month['month']=='09']['day'])\ncount2 = Counter(day_and_month[day_and_month['month']=='10']['day'])\n\nkeys = pd.concat([pd.Series(count1.keys()), pd.Series(count2.keys())[:2]])\nvalues = pd.concat([pd.Series(count1.values()), pd.Series(count2.values())[:2]])\ncount = dict(zip(keys, values))\n\nplots('Articles released over the days', 'Days', 'Number of articles', count.values(),\n      count.keys(), (13, 8))","metadata":{"execution":{"iopub.status.busy":"2021-06-09T16:56:32.393525Z","iopub.execute_input":"2021-06-09T16:56:32.393905Z","iopub.status.idle":"2021-06-09T16:56:32.593175Z","shell.execute_reply.started":"2021-06-09T16:56:32.393874Z","shell.execute_reply":"2021-06-09T16:56:32.592307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Analyzing Engagement ","metadata":{}},{"cell_type":"code","source":"plots('Engagement reaction count over time', 'Time', 'Engagement reaction count', \n      df['engagement_reaction_count'], df['engagement_reaction_count'].keys(), (13, 8))","metadata":{"execution":{"iopub.status.busy":"2021-06-09T16:58:27.60891Z","iopub.execute_input":"2021-06-09T16:58:27.609314Z","iopub.status.idle":"2021-06-09T16:58:27.791176Z","shell.execute_reply.started":"2021-06-09T16:58:27.609281Z","shell.execute_reply":"2021-06-09T16:58:27.790242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plots('Engagement comment count over time', 'Time', 'Engagement comment count', \n      df['engagement_comment_count'], df['engagement_comment_count'].keys(), (13, 8))","metadata":{"execution":{"iopub.status.busy":"2021-06-09T16:58:28.341104Z","iopub.execute_input":"2021-06-09T16:58:28.341681Z","iopub.status.idle":"2021-06-09T16:58:28.534642Z","shell.execute_reply.started":"2021-06-09T16:58:28.341627Z","shell.execute_reply":"2021-06-09T16:58:28.533568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plots('Engagement share count over time', 'Time', 'Engagement share count', \n      df['engagement_share_count'], df['engagement_share_count'].keys(), (13, 8))","metadata":{"execution":{"iopub.status.busy":"2021-06-09T16:58:30.648167Z","iopub.execute_input":"2021-06-09T16:58:30.648566Z","iopub.status.idle":"2021-06-09T16:58:30.835461Z","shell.execute_reply.started":"2021-06-09T16:58:30.648532Z","shell.execute_reply":"2021-06-09T16:58:30.83423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plots('Engagement comment plugin count over time', 'Time', 'Engagement comment plugin count', \n      df['engagement_comment_plugin_count'], df['engagement_comment_plugin_count'].keys(), \n      (13, 8))","metadata":{"execution":{"iopub.status.busy":"2021-06-09T16:58:31.387324Z","iopub.execute_input":"2021-06-09T16:58:31.387702Z","iopub.status.idle":"2021-06-09T16:58:31.561293Z","shell.execute_reply.started":"2021-06-09T16:58:31.387666Z","shell.execute_reply":"2021-06-09T16:58:31.560294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Correlations","metadata":{}},{"cell_type":"markdown","source":"I'm going to use a heatmap to examine whether or not there are correlations between any of the variables. There are three different types of features that have a connection, so there likely is a dependency on one another:","metadata":{}},{"cell_type":"code","source":"sns.heatmap(df.corr(), annot=True)\nplt.title('Correlation of variables')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-09T17:01:12.770627Z","iopub.execute_input":"2021-06-09T17:01:12.771052Z","iopub.status.idle":"2021-06-09T17:01:13.379759Z","shell.execute_reply.started":"2021-06-09T17:01:12.771002Z","shell.execute_reply":"2021-06-09T17:01:13.378635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's scatter the datapoints and create a line that best fit to see how the different variables correlate to one another. Let's all remove some of the outliers in the columns because they could skew our results:","metadata":{}},{"cell_type":"code","source":"df['engagement_reaction_count'] = df['engagement_reaction_count'][df['engagement_reaction_count']<100000]\ndf['engagement_share_count'] = df['engagement_share_count'][df['engagement_share_count']<20000]","metadata":{"execution":{"iopub.status.busy":"2021-06-09T17:02:17.407548Z","iopub.execute_input":"2021-06-09T17:02:17.407879Z","iopub.status.idle":"2021-06-09T17:02:17.432719Z","shell.execute_reply.started":"2021-06-09T17:02:17.407851Z","shell.execute_reply":"2021-06-09T17:02:17.431969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fontsize=15\nfig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 7))\n\nfor ax in [[ax1, ['engagement_reaction_count', 'engagement_comment_count']],\n           [ax2, ['engagement_reaction_count', 'engagement_share_count']],\n           [ax3, ['engagement_share_count', 'engagement_comment_count']]]:\n    sns.regplot(data=df, x=ax[1][0], y=ax[1][1], ax=ax[0])\n    ax[0].set_xlabel(ax[1][0], fontsize=fontsize)\n    ax[0].set_ylabel(ax[1][1], fontsize=fontsize)\n\nax2.set_title('Correlation of variables', fontsize=30, pad=30)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-09T17:02:25.797785Z","iopub.execute_input":"2021-06-09T17:02:25.798303Z","iopub.status.idle":"2021-06-09T17:02:28.219051Z","shell.execute_reply.started":"2021-06-09T17:02:25.798271Z","shell.execute_reply":"2021-06-09T17:02:28.218058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predicting the data (pt. 2)","metadata":{}},{"cell_type":"markdown","source":"## Let's split up our dataset:","metadata":{}},{"cell_type":"markdown","source":"We can assign 'X\" to our 'content' feature and 'y' to our 'source_name' feature, which we can further split into our train and test sets ","metadata":{}},{"cell_type":"code","source":"X = df['content']\ny = df['source_name']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","metadata":{"execution":{"iopub.status.busy":"2021-06-09T17:04:14.216032Z","iopub.execute_input":"2021-06-09T17:04:14.216407Z","iopub.status.idle":"2021-06-09T17:04:14.224508Z","shell.execute_reply.started":"2021-06-09T17:04:14.216376Z","shell.execute_reply":"2021-06-09T17:04:14.223534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## NLP","metadata":{}},{"cell_type":"markdown","source":"Let's convert our data from textual into numerical format to input it into a predictor. We can do so using \"CountVectorizer\" and a \"TFIDF\" model.","metadata":{}},{"cell_type":"code","source":"cv = CountVectorizer()\ntfidf = TfidfTransformer()\n\nX_train = cv.fit_transform(X_train)\nX_test = cv.transform(X_test)\n\nX_train = tfidf.fit_transform(X_train)\nX_test = tfidf.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-06-09T17:07:54.379126Z","iopub.execute_input":"2021-06-09T17:07:54.379507Z","iopub.status.idle":"2021-06-09T17:07:54.979841Z","shell.execute_reply.started":"2021-06-09T17:07:54.379476Z","shell.execute_reply":"2021-06-09T17:07:54.978722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Using TruncatedSVD","metadata":{}},{"cell_type":"markdown","source":"We can reduce the unwanted parts of the data through a TruncatedSVD model, which essentially is a PCA for our text:","metadata":{}},{"cell_type":"code","source":"svd = TruncatedSVD(n_components=2000)\nX_train = svd.fit_transform(X_train)\nX_test = svd.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-06-09T17:08:45.237247Z","iopub.execute_input":"2021-06-09T17:08:45.237853Z","iopub.status.idle":"2021-06-09T17:09:49.52924Z","shell.execute_reply.started":"2021-06-09T17:08:45.237806Z","shell.execute_reply":"2021-06-09T17:09:49.528373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Let's create and evaluate our classifiers: 'SGD', 'Random Forest', and 'Linear SVC'.","metadata":{}},{"cell_type":"code","source":"classifiers = [['SGD', SGDClassifier()], ['Random Forest', RandomForestClassifier()],\n              ['Linear SVC', LinearSVC()]]\nscores = []\ncross_vals = []\n\nfor classifier in classifiers:\n    model = classifier[1]\n    model.fit(X_train, y_train)\n\n    score = model.score(X_test, y_test)\n    cross_val = cross_val_score(model, X_test, y_test).mean()\n    scores.append(score)\n    cross_vals.append(cross_val)\n    \n    print(classifier[0])\n    print(score)\n    print(cross_val)\n    if model != classifiers[-1][1]:\n        print('')","metadata":{"execution":{"iopub.status.busy":"2021-06-09T17:09:49.531621Z","iopub.execute_input":"2021-06-09T17:09:49.532089Z","iopub.status.idle":"2021-06-09T17:11:49.298166Z","shell.execute_reply.started":"2021-06-09T17:09:49.532044Z","shell.execute_reply":"2021-06-09T17:11:49.295468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lastly, let's use bar charts to visualize how well each classifier did in relation to a model score and cross val score.","metadata":{}},{"cell_type":"code","source":"names = ['SGD', 'Random Forest', 'Linear SVC']\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(13, 9))\n\nfor ax in [[ax1, scores, 'model score'], [ax2, cross_vals, 'cross validation score']]:\n    metric = ax[1]\n    bars = ax[0].bar(names, metric, color='blue')\n    for bar in bars:\n        label = str(metric[list(bars).index(bar)])[:4]\n        height = bar.get_height()\n        ax[0].text(bar.get_x() + bar.get_width()/2, height, label, ha='center', va='bottom')\n    ax[0].set_title(ax[2])\n    ax[0].set_xlabel('model')\n    ax[0].set_ylabel('accuracy')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-09T17:11:49.300429Z","iopub.execute_input":"2021-06-09T17:11:49.300891Z","iopub.status.idle":"2021-06-09T17:11:49.594688Z","shell.execute_reply.started":"2021-06-09T17:11:49.300845Z","shell.execute_reply":"2021-06-09T17:11:49.593872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"LinearSVC did the best out of all of our classifiers.","metadata":{}},{"cell_type":"markdown","source":"# Conclusion","metadata":{}},{"cell_type":"markdown","source":"### This notebook created a reccomendation engine and used EDA and NLP to examine and predict trends in our data about internet news.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}